openapi: 3.1.0
info:
  title: LLM Service API (Private)
  version: 1.0.0
  description: Token-streaming text generation with primary OpenAI and fallback HF/TGI or vLLM.
servers:
  - url: http://llm:8000
security:
  - bearerAuth: []
tags:
  - name: System
  - name: Generation
components:
  securitySchemes:
    bearerAuth: { type: http, scheme: bearer, bearerFormat: JWT }
  parameters:
    XSessionId: { name: X-Session-Id, in: header, required: true, schema: { type: string } }
    XRequestId: { name: X-Request-Id, in: header, required: true, schema: { type: string } }
    XCorrelationId: { name: X-Correlation-Id, in: header, required: true, schema: { type: string } }
  schemas:
    Error:
      type: object
      required: [code, message, correlation_id]
      properties:
        code: { type: string }
        message: { type: string }
        details: { type: object, additionalProperties: true }
        correlation_id: { type: string }
    Message:
      type: object
      required: [role, content]
      properties:
        role: { type: string, enum: [system, user, assistant] }
        content: { type: string }
    GenerateRequest:
      type: object
      required: [messages]
      properties:
        messages:
          type: array
          items: { $ref: "#/components/schemas/Message" }
        temperature: { type: number, default: 0.2 }
        max_tokens: { type: integer, default: 512, maximum: 2000 }
        provider_hint: { type: string, enum: [openai, hf], description: "Optional hint" }
      example:
        messages:
          - role: user
            content: "What is the role of ClickHouse in analytics?"
        temperature: 0.1
    SSEExample:
      type: string
      example: |
        event: llm.token
        data: {"delta":"ClickHouse "}

        event: llm.token
        data: {"delta":"stores events..."}

        event: llm.done
        data: {"provider":"openai","model":"gpt-4o","fallback_used":false,"usage":{"prompt_tokens":64,"completion_tokens":40}}
paths:
  /v1/generate:
    post:
      tags: [Generation]
      summary: Streamed text generation (SSE)
      description: Streams `llm.token` and finishes with `llm.done`. On provider switch, continues mid-stream.
      parameters:
        - $ref: "#/components/parameters/XSessionId"
        - $ref: "#/components/parameters/XRequestId"
        - $ref: "#/components/parameters/XCorrelationId"
      requestBody:
        required: true
        content:
          application/json:
            schema: { $ref: "#/components/schemas/GenerateRequest" }
      responses:
        "200":
          description: SSE tokens and done event
          content:
            text/event-stream:
              schema: { $ref: "#/components/schemas/SSEExample" }
        "400":
          description: Bad request
          content:
            application/json: { schema: { $ref: "#/components/schemas/Error" } }
  /v1/health:
    get:
      tags: [System]
      summary: Health
      security: []
      parameters: [ { $ref: "#/components/parameters/XRequestId" } ]
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema: { type: object, example: { status: ok } }
  /v1/config:
    get:
      tags: [System]
      summary: Config (providers, models)
      parameters: [ { $ref: "#/components/parameters/XSessionId" }, { $ref: "#/components/parameters/XRequestId" }, { $ref: "#/components/parameters/XCorrelationId" } ]
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                type: object
                example:
                  openai_model: "gpt-4o"
                  fallback: "hf/tgi"
                  first_token_sla_ms: 1000
  /v1/metrics:
    get:
      tags: [System]
      summary: Prometheus metrics
      security: []
      parameters: [ { $ref: "#/components/parameters/XRequestId" } ]
      responses:
        "200":
          description: Text
          content:
            text/plain:
              schema: { type: string }
              example: "llm_first_token_ms_sum 1234"
